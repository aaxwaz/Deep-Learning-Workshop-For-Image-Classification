{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUS Deep Learning workshop on computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow\n",
    "* A system for executing computational graphs over Tensor objects - n-dimensional arrays analogous to the numpy ndarray.\n",
    "* Native support for performing backpropogation for its Variables. \n",
    "* Used by Google for both research and production. \n",
    "\n",
    "### Why use Tensorflow\n",
    "\n",
    "* Save you a lot of time when buiding large computational graphs - can automatically compute gradients to update weights!\n",
    "* Code can be run on GPU - usually 5x - 10x times faster than CPU for usually image networks. And you don't need to worry about low level cuda-code, things are taken care for you in tf. \n",
    "* A lot of good high level APIs or pre-trained models that you can use directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief example of tensorflow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph building blocks: \n",
    "1) Placeholder: input nodes to our graph, which serve as entry points where we can feed data into graph - `tf.placeholder`\n",
    "\n",
    "2) Variables: tensors that hold values in the graph, which will be updated during training - `tf.Variable`\n",
    "\n",
    "3) Ops: arithmetic operators, as well as activation, convolution, loss ops and many more - \n",
    "* a = tf.constant([3, 6])\n",
    "* b = tf.constant([2, 2])\n",
    "* tf.add(a, b) # >> [5 8]\n",
    "* tf.add_n([a, b, b]) # >> [7 10]. Equivalent to a + b + b\n",
    "* tf.mul(a, b) # >> [6 12] because mul is element wise\n",
    "* tf.matmul(a, b) # >> ValueError\n",
    "* tf.matmul(tf.reshape(a, shape=[1, 2]), tf.reshape(b, shape=[2, 1])) # >> [[18]]\n",
    "* tf.div(a, b) # >> [1 3]\n",
    "* tf.mod(a, b) # >> [1 0]\n",
    "\n",
    "* tf.nn*, such as tf.nn.conv2d, tf.nn.max_pool, tf.nn.softmax, tf.nn.relu, and so on\n",
    "\n",
    "4) tf.train* subclasses: Optimizer base classes that compute gradients for a loss and apply gradients to variables. Such as tf.train.GradientDescentOptimizer, tf.train.AdadeltaOptimizer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "N = 128 # sample size \n",
    "D = 5 # data dimension \n",
    "H = 2 # hidden layer dimension \n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D), name = 'input')\n",
    "y = tf.placeholder(tf.float32, shape=(N, 1), name = 'target')\n",
    "\n",
    "# all weights for model \n",
    "w1 = tf.Variable(tf.random_normal((D, H)), name = 'weights1')\n",
    "b1 = tf.Variable(tf.zeros([2]), name = 'biases1')\n",
    "w2 = tf.Variable(tf.random_normal((H, 1)), name = 'weights2')\n",
    "b2 = tf.Variable(tf.zeros([1]), name = 'biases1')\n",
    "\n",
    "# relu hidden layer\n",
    "h = tf.maximum(tf.matmul(x, w1) + b1, 0)\n",
    "\n",
    "# output layer \n",
    "y_pred = tf.matmul(h, w2) + b2\n",
    "\n",
    "# loss \n",
    "loss = tf.reduce_mean((y_pred - y)**2)\n",
    "\n",
    "# update \n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "updates = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'weights1:0' shape=(5, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'biases1:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'weights2:0' shape=(2, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'biases1_1:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_x = np.random.randn(N, D)\n",
    "data_y = np.random.randn(N, 1)\n",
    "\n",
    "data = {x: data_x, \n",
    "        y: data_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    losses = []\n",
    "    for t in range(1000):\n",
    "        loss_val, _  = sess.run([loss, updates], \n",
    "                                feed_dict = data)\n",
    "        losses.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0403604780>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04040b95c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Can switch to higher level layers too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D), name = 'input')\n",
    "y = tf.placeholder(tf.float32, shape=(N, 1), name = 'target')\n",
    "\n",
    "# weights initializer\n",
    "init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# hidden layer\n",
    "h = tf.layers.dense(inputs=x, units=H, \n",
    "                    activation=tf.nn.relu, \n",
    "                    kernel_initializer = init, \n",
    "                    name='hidden_layer')\n",
    "\n",
    "# output layer\n",
    "y_pred = tf.layers.dense(inputs=h, units=1, \n",
    "                        kernel_initializer = init, \n",
    "                        name='output_layer')\n",
    "\n",
    "# loss\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "\n",
    "# updates\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "updates = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {x: data_x, \n",
    "        y: data_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    losses = []\n",
    "    for t in range(1000):\n",
    "        loss_val, _  = sess.run([loss, updates], \n",
    "                                feed_dict = data)\n",
    "        losses.append(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0403492630>]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nu4iIDyncRUR8SOEuIuJD/w3uIKdwkzFPHwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f04040a4b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Save and test your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save - save all the weights of our graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss is: 0.9703320264816284 and model saved at: /home/weimin/workshop/testing.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "data = {x: data_x, \n",
    "        y: data_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    losses = []\n",
    "    for t in range(1000):\n",
    "        loss_val, _  = sess.run([loss, updates], \n",
    "                                feed_dict = data)\n",
    "        losses.append(loss_val)\n",
    "        \n",
    "    saved_path = saver.save(sess, '/home/weimin/workshop/testing.ckpt')\n",
    "print(\"Final loss is: {} and model saved at: {}\".format(losses[-1], saved_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/weimin/workshop/testing.ckpt\n",
      "0.97033\n"
     ]
    }
   ],
   "source": [
    "# First, we have to rebuild the graph\n",
    "# reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D), name = 'input')\n",
    "y = tf.placeholder(tf.float32, shape=(N, 1), name = 'target')\n",
    "\n",
    "# weights initializer\n",
    "init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# hidden layer\n",
    "h = tf.layers.dense(inputs=x, units=H, \n",
    "                    activation=tf.nn.relu, \n",
    "                    kernel_initializer = init, \n",
    "                    name='hidden_layer')\n",
    "\n",
    "# output layer\n",
    "y_pred = tf.layers.dense(inputs=h, units=1, \n",
    "                        kernel_initializer = init, \n",
    "                        name='output_layer')\n",
    "\n",
    "# loss\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "# Second, load the weights from your previously trained model, and test on your data\n",
    "data = {x: data_x, \n",
    "        y: data_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, saved_path)\n",
    "    \n",
    "    final_loss = sess.run(loss, feed_dict = data)\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an image classifier using CNN\n",
    "\n",
    "- `tf.nn.conv2d`: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
    "- `tf.layers.conv2d`: https://www.tensorflow.org/api_docs/python/tf/layers/conv2d\n",
    "- `tf.contrib.layers.convolution2d`: https://www.tensorflow.org/api_guides/python/contrib.layers (not covered)\n",
    "- `tf.nn.max_pool`: https://www.tensorflow.org/api_docs/python/tf/nn/max_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) The hard way - using `tf.nn.conv2d` directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# input image shape is [batch_size, height, width, channels]\n",
    "input_image = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "\n",
    "def conv2d(x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def weight_variable(shape, name = None):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name = name)\n",
    "\n",
    "def bias_variable(shape, name = None):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name = name)\n",
    "\n",
    "W_conv = weight_variable([5, 5, 3, 64], name = 'W') # [filter_height, filter_width, in_channels, out_channels]\n",
    "b_conv = bias_variable([64], name = 'b')\n",
    "h_conv = tf.nn.relu(conv2d(input_image, W_conv) + b_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(?, 32, 32, 64) dtype=float32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'W:0' shape=(5, 5, 3, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'b:0' shape=(64,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) The easy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# input image shape is [batch_size, height, width, channels]\n",
    "input_image = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "\n",
    "h_conv = tf.layers.conv2d(\n",
    "  inputs=input_image,\n",
    "  filters=64,\n",
    "  kernel_size=[5, 5],\n",
    "  padding=\"same\",\n",
    "  kernel_initializer=tf.truncated_normal_initializer,\n",
    "  activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d/Relu:0' shape=(?, 32, 32, 64) dtype=float32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d/kernel:0' shape=(5, 5, 3, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d/bias:0' shape=(64,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(1, 2, 2, 2) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1 = tf.nn.max_pool(h_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "h_pool1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - CNN of a toy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_2:0' shape=(1, 4, 4, 3) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_input = tf.constant([\n",
    "            [\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]], \n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]]\n",
    "            ]\n",
    "        ])\n",
    "image_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv = tf.layers.conv2d(\n",
    "  inputs=image_input,\n",
    "  filters=2,\n",
    "  kernel_size=[1, 1],\n",
    "  padding=\"same\",\n",
    "  kernel_initializer=tf.truncated_normal_initializer,\n",
    "  activation=tf.nn.relu)\n",
    "h_pool = tf.layers.max_pooling2d(inputs=h_conv, pool_size=[2, 2], strides=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[   1.29909432    0.        ]\n",
      "   [   1.29909432    0.        ]\n",
      "   [   1.29909432    0.        ]\n",
      "   [   1.29909432    0.        ]]\n",
      "\n",
      "  [[   1.29909432    0.        ]\n",
      "   [   1.29909432    0.        ]\n",
      "   [   1.29909432    0.        ]\n",
      "   [   1.29909432    0.        ]]\n",
      "\n",
      "  [[ 309.97921753   85.61038208]\n",
      "   [ 331.26904297    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]\n",
      "\n",
      "  [[ 309.97921753   85.61038208]\n",
      "   [ 331.26904297    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]]]\n",
      "Output tensor shape:  (1, 4, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    h_conv_res = sess.run(h_conv)\n",
    "    print(h_conv_res)\n",
    "    print(\"Output tensor shape: \", h_conv_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'max_pooling2d/MaxPool:0' shape=(1, 2, 2, 2) dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Flatten last pooling 1-D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten_dim = h_pool.get_shape().as_list()[1] * h_pool.get_shape().as_list()[2] * h_pool.get_shape().as_list()[3]\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, flatten_dim])\n",
    "h_pool_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) FC layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([flatten_dim, 2], name = 'W')\n",
    "    b_fc1 = bias_variable([2], name = 'b')\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(1, 2) dtype=float32>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Batch norm (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(h, is_training):\n",
    "    return tf.contrib.layers.batch_norm(h, \n",
    "                                        center=True, \n",
    "                                        scale=True, \n",
    "                                        is_training=is_training)\n",
    "h_fc1_batchnorm = batch_norm(h_fc1, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=tf.constant([1,0]),\n",
    "                                               logits=h_fc1)\n",
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Training Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'W:0' shape=(5, 5, 3, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'b:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d/kernel:0' shape=(1, 1, 3, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'fc1/W:0' shape=(8, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'fc1/b:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'BatchNorm/beta:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'BatchNorm/gamma:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Prediction and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) The whole script for building graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(config):\n",
    "    \"\"\"This function builds the graph for a deep net for classifying images.\n",
    "    Args:\n",
    "      config: Model configuration object\n",
    "    Returns:\n",
    "      A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "      equal to the logits of classifying the digit into one of 10 classes (the\n",
    "      digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "      dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    x_image = tf.placeholder(tf.float32, [None, config.image_height, config.image_width, config.image_channels])\n",
    "    y = tf.placeholder(tf.float32, [None, int(config.num_classes)])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # First convolutional layer - maps one grayscale image to 32 feature maps.\n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([config.filter_size, config.filter_size, config.image_channels, config.conv1_num_filters], name = 'W')\n",
    "        b_conv1 = bias_variable([config.conv1_num_filters], name = 'b')\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "    # Pooling layer - downsamples by 2X.\n",
    "    with tf.name_scope('pool1'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "    # Second convolutional layer -- maps 32 feature maps to 64.\n",
    "    with tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([config.filter_size, config.filter_size, config.conv1_num_filters, config.conv2_num_filters], name = 'W')\n",
    "        b_conv2 = bias_variable([config.conv2_num_filters], name = 'b')\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "    # Second pooling layer.\n",
    "    with tf.name_scope('pool2'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    # Fully connected layer 1 -- after 2 round of downsampling, our 32x32 image\n",
    "    # is down to 8x8x64 feature maps -- maps this to 1024 features.\n",
    "    feature_map_flattened_dim = int((config.image_height/(2*2)) * (config.image_width/(2*2)) * config.conv2_num_filters)\n",
    "    with tf.name_scope('fc1'):\n",
    "        W_fc1 = weight_variable([feature_map_flattened_dim, config.fc1_num_features], name = 'W')\n",
    "        b_fc1 = bias_variable([config.fc1_num_features], name = 'b')\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, feature_map_flattened_dim])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "    # Dropout - controls the complexity of the model, prevents co-adaptation of\n",
    "    # features.\n",
    "    with tf.name_scope('dropout'):\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Map the 1024 features to 10 classes, one for each digit\n",
    "    with tf.name_scope('fc2'):\n",
    "        W_fc2 = weight_variable([config.fc1_num_features, config.num_classes], name = 'W')\n",
    "        b_fc2 = bias_variable([config.num_classes], name = 'b')\n",
    "\n",
    "    # Raw predictions - logits\n",
    "    with tf.name_scope('logits'):\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "    with tf.name_scope('probabilities'):\n",
    "        probs = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                       logits=logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    with tf.name_scope('adam_optimizer'):\n",
    "        train_step = tf.train.AdamOptimizer(config.learning_rate).minimize(loss)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "    saver = tf.train.Saver\n",
    "\n",
    "    # Return the model in dict\n",
    "    return dict(\n",
    "        x_image = x_image, \n",
    "        y = y, \n",
    "        keep_prob = keep_prob, \n",
    "        logits = logits, \n",
    "        probs = probs, \n",
    "        loss = loss, \n",
    "        train_step = train_step, \n",
    "        accuracy = accuracy, \n",
    "        saver = saver, \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12) Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_for_one_epoch(iterations, train_x, train_y, model, sess):\n",
    "\n",
    "    num_images = len(train_x)\n",
    "    for _ in range(iterations):\n",
    "        # Create a random index.\n",
    "        idx = np.random.choice(num_images,\n",
    "                               size=256,\n",
    "                               replace=False)\n",
    "        batch_x = train_x[idx, :, :, :]\n",
    "        batch_y = train_y[idx, :]\n",
    "\n",
    "        if record_train_loss: \n",
    "            _, temp_loss = sess.run([model['train_step'], model['loss']], feed_dict={model['x_image']: batch_x, \\\n",
    "                                                                                     model['y']: batch_y, \\\n",
    "                                                                                     model['keep_prob']:0.5})\n",
    "            training_loss.append(temp_loss)\n",
    "        else:\n",
    "            sess.run(model['train_step'], feed_dict={model['x_image']: batch_x, \\\n",
    "                                                     model['y']: batch_y, \\\n",
    "                                                     model['keep_prob']:0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13) Model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_prediction(val_x, val_y, model, sess):\n",
    "\n",
    "    val_accur = sess.run(model['accuracy'], feed_dict={model['x_image']: val_x, \\\n",
    "                                                       model['y']: val_y, \\\n",
    "                                                       model['keep_prob']: 1.0})\n",
    "return val_accur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14) Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & val losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/train_losses.png\" />\n",
    "\n",
    "<img src=\"images/val_losses_n_accuracy.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize activations from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "idx = np.random.choice(train_data.shape[0],\n",
    "      size=1,\n",
    "      replace=False)\n",
    "img = train_data[idx[0], :, :, :]\n",
    "## visualize layer-1 kernel weights in grid \n",
    "img = np.expand_dims(img, 0)\n",
    "h_conv1_1 = sess.run(model['h_conv1_1'], feed_dict={model['x_image']:img})\n",
    "h_conv1_1 = h_conv1_1.transpose(3, 1, 2, 0)   # reshape to: (N, H, W, 1)\n",
    "vis_grid = visualize_grid(h_conv1_1, grey = True)\n",
    "plot_weights_in_grid(vis_grid, os.path.join(save_dir, 'vis_activations.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td><img src=\"images/vis_acti_dog.png\" /></td>\n",
    "<td><img src=\"images/vis_acti_frog.png\" /></td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the files: \n",
    "****`train.py`:****  To build & train the model, as well as to monitor the training progress and to visualize graphs like above. Upon completion, it will save the trained model into specified location. How to run:                        \n",
    "`python train.py --trainDir /home/ubuntu/workshop/ --savedSessionDir /home/ubuntu/workshop/savedSessions/`\n",
    "\n",
    "**** `build_graph.py`: ****  Build the model graph and return list of nodes to the graph\n",
    "\n",
    "**** `configuration.py`: **** Configuration file for model and training. \n",
    "\n",
    "**** `vis_utils.py`: **** Visualization utility functions\n",
    "\n",
    "**** `data_utils.py`: **** Data utility functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
