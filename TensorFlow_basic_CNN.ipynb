{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUS Deep Learning workshop on computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow\n",
    "* A system for executing computational graphs over Tensor objects - n-dimensional arrays analogous to the numpy ndarray.\n",
    "* Native support for performing backpropogation for its Variables. \n",
    "* Used by Google for both research and production. \n",
    "\n",
    "### Why use Tensorflow\n",
    "\n",
    "* Save you a lot of time when buiding large computational graphs - can automatically compute gradients to update weights!\n",
    "* Code can be run on GPU - usually 5x - 20x times faster than CPU for usually image networks. And you don't need to worry about low level cuda-code, things are taken care for you in tf. \n",
    "* A lot of good high level APIs or pre-trained models that you can use directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief example of tensorflow  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph building blocks: \n",
    "1) Placeholder: input nodes to our graph, which serve as entry points where we can feed data into graph - `tf.placeholder`\n",
    "\n",
    "2) Variables: tensors that hold values in the graph, which will be updated during training - `tf.Variable`\n",
    "\n",
    "3) Ops: arithmetic operators, as well as activation, convolution, loss ops and many more - \n",
    "* a = tf.constant([3, 6])\n",
    "* b = tf.constant([2, 2])\n",
    "* tf.add(a, b) # >> [5 8]\n",
    "* tf.add_n([a, b, b]) # >> [7 10]. Equivalent to a + b + b\n",
    "* tf.mul(a, b) # >> [6 12] because mul is element wise\n",
    "* tf.matmul(a, b) # >> ValueError\n",
    "* tf.matmul(tf.reshape(a, shape=[1, 2]), tf.reshape(b, shape=[2, 1])) # >> [[18]]\n",
    "* tf.div(a, b) # >> [1 3]\n",
    "* tf.mod(a, b) # >> [1 0]\n",
    "\n",
    "* tf.nn*, such as tf.nn.conv2d, tf.nn.max_pool, tf.nn.softmax, tf.nn.relu, and so on\n",
    "\n",
    "4) tf.train* subclasses: Optimizer base classes that compute gradients for a loss and apply gradients to variables. Such as tf.train.GradientDescentOptimizer, tf.train.AdadeltaOptimizer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Build the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d485588>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAADGCAYAAADc30sqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpwEeiGEiHAS6IUQIsL9P0ttCpDSc3GfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11265efd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "img = plt.imread('./images/LM.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "N = 128 # sample size \n",
    "D = 5 # data dimension \n",
    "H = 2 # hidden layer dimension \n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D), name = 'input')\n",
    "y = tf.placeholder(tf.float32, shape=(N, 1), name = 'target')\n",
    "\n",
    "# all weights for model \n",
    "w1 = tf.Variable(tf.random_normal((D, H)), name = 'weights1')\n",
    "b1 = tf.Variable(tf.zeros([2]), name = 'biases1')\n",
    "w2 = tf.Variable(tf.random_normal((H, 1)), name = 'weights2')\n",
    "b2 = tf.Variable(tf.zeros([1]), name = 'biases1')\n",
    "\n",
    "# relu hidden layer\n",
    "h = tf.maximum(tf.matmul(x, w1) + b1, 0)\n",
    "\n",
    "# output layer \n",
    "y_pred = tf.matmul(h, w2) + b2\n",
    "\n",
    "# loss \n",
    "loss = tf.reduce_mean((y_pred - y)**2)\n",
    "\n",
    "# update \n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "updates = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'weights1:0' shape=(5, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'biases1:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'weights2:0' shape=(2, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'biases1_1:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_x = np.random.randn(N, D)\n",
    "data_y = np.random.randn(N, 1)\n",
    "\n",
    "data = {x: data_x, \n",
    "        y: data_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    losses = []\n",
    "    for t in range(1000):\n",
    "        loss_val, _  = sess.run([loss, updates], \n",
    "                                feed_dict = data)\n",
    "        losses.append(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1279c9780>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e1a7438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Can switch to high-level layers too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D), name = 'input')\n",
    "y = tf.placeholder(tf.float32, shape=(N, 1), name = 'target')\n",
    "\n",
    "# weights initializer\n",
    "init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# hidden layer\n",
    "h = tf.layers.dense(inputs=x, units=H, \n",
    "                    activation=tf.nn.relu, \n",
    "                    kernel_initializer = init, \n",
    "                    name='hidden_layer')\n",
    "\n",
    "# output layer\n",
    "y_pred = tf.layers.dense(inputs=h, units=1, \n",
    "                        kernel_initializer = init, \n",
    "                        name='output_layer')\n",
    "\n",
    "# loss\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "\n",
    "# updates\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "updates = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden_layer/kernel:0' shape=(5, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden_layer/bias:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'output_layer/kernel:0' shape=(2, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'output_layer/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {x: data_x, \n",
    "        y: data_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    losses = []\n",
    "    for t in range(1000):\n",
    "        loss_val, _  = sess.run([loss, updates], \n",
    "                                feed_dict = data)\n",
    "        losses.append(loss_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12c398780>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nh/4/fT+elI0YjsUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1279c9dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Save and test your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and save - save all the weights of our graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss is: 0.9991867542266846 and model saved at: /Users/weimin/Desktop/workshop/testing.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "data = {x: data_x, \n",
    "        y: data_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    losses = []\n",
    "    for t in range(1000):\n",
    "        loss_val, _  = sess.run([loss, updates], \n",
    "                                feed_dict = data)\n",
    "        losses.append(loss_val)\n",
    "        \n",
    "    saved_path = saver.save(sess, '/Users/weimin/Desktop/workshop/testing.ckpt')\n",
    "print(\"Final loss is: {} and model saved at: {}\".format(losses[-1], saved_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/weimin/Desktop/workshop/testing.ckpt\n",
      "0.999152\n"
     ]
    }
   ],
   "source": [
    "# First, we have to rebuild the graph\n",
    "# reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(N, D), name = 'input')\n",
    "y = tf.placeholder(tf.float32, shape=(N, 1), name = 'target')\n",
    "\n",
    "# weights initializer\n",
    "init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "# hidden layer\n",
    "h = tf.layers.dense(inputs=x, units=H, \n",
    "                    activation=tf.nn.relu, \n",
    "                    kernel_initializer = init, \n",
    "                    name='hidden_layer')\n",
    "\n",
    "# output layer\n",
    "y_pred = tf.layers.dense(inputs=h, units=1, \n",
    "                        kernel_initializer = init, \n",
    "                        name='output_layer')\n",
    "\n",
    "# loss\n",
    "loss = tf.losses.mean_squared_error(y_pred, y)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "# Second, load the weights from your previously trained model, and test on your data\n",
    "data = {x: data_x, \n",
    "        y: data_y}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore model weights from previously saved model\n",
    "    saver.restore(sess, saved_path)\n",
    "    \n",
    "    final_loss = sess.run(loss, feed_dict = data)\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an image classifier using CNN\n",
    "\n",
    "- `tf.nn.conv2d`: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
    "- `tf.layers.conv2d`: https://www.tensorflow.org/api_docs/python/tf/layers/conv2d\n",
    "- `tf.contrib.layers.convolution2d`: https://www.tensorflow.org/api_guides/python/contrib.layers (not covered)\n",
    "- `tf.nn.max_pool`: https://www.tensorflow.org/api_docs/python/tf/nn/max_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) The hard way - using `tf.nn.conv2d` directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# input image shape is [batch_size, height, width, channels]\n",
    "input_image = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "\n",
    "def conv2d(x, W):\n",
    "  \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "  return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def weight_variable(shape, name = None):\n",
    "  \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial, name = name)\n",
    "\n",
    "def bias_variable(shape, name = None):\n",
    "  \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial, name = name)\n",
    "\n",
    "W_conv = weight_variable([5, 5, 3, 64], name = 'W') # [filter_height, filter_width, in_channels, out_channels]\n",
    "b_conv = bias_variable([64], name = 'b')\n",
    "h_conv = tf.nn.relu(conv2d(input_image, W_conv) + b_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu:0' shape=(?, 32, 32, 64) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'W:0' shape=(5, 5, 3, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'b:0' shape=(64,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) The easy way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# input image shape is [batch_size, height, width, channels]\n",
    "input_image = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "\n",
    "h_conv = tf.layers.conv2d(\n",
    "  inputs=input_image,\n",
    "  filters=64,\n",
    "  kernel_size=[5, 5],\n",
    "  padding=\"same\",\n",
    "  kernel_initializer=tf.truncated_normal_initializer,\n",
    "  activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv2d/Relu:0' shape=(?, 32, 32, 64) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d/kernel:0' shape=(5, 5, 3, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d/bias:0' shape=(64,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1 = tf.nn.max_pool(h_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "h_pool1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - CNN of a toy image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input - 5 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(5, 4, 4, 3) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "image_input = tf.constant([\n",
    "            [\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]], \n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]]\n",
    "            ], \n",
    "    \n",
    "            [\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]], \n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]]\n",
    "            ], \n",
    "    \n",
    "            [\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]], \n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]]\n",
    "            ], \n",
    "            [\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]], \n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]]\n",
    "            ], \n",
    "            [\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]],\n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]], \n",
    "                [[254., 0., 0.], [255., 255., 255.], [0., 0., 0.], [0., 0., 0.]]\n",
    "            ]\n",
    "        ])\n",
    "image_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_conv = tf.layers.conv2d(\n",
    "  inputs=image_input,\n",
    "  filters=2,\n",
    "  kernel_size=[1, 1],\n",
    "  padding=\"same\",\n",
    "  kernel_initializer=tf.truncated_normal_initializer,\n",
    "  activation=tf.nn.relu)\n",
    "h_pool = tf.layers.max_pooling2d(inputs=h_conv, pool_size=[2, 2], strides=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]]\n",
      "\n",
      "\n",
      " [[[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]]\n",
      "\n",
      "\n",
      " [[[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]]\n",
      "\n",
      "\n",
      " [[[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]]\n",
      "\n",
      "\n",
      " [[[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]\n",
      "   [   1.06532812    0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]\n",
      "\n",
      "  [[   0.            0.        ]\n",
      "   [ 271.65869141    0.        ]\n",
      "   [   0.            0.        ]\n",
      "   [   0.            0.        ]]]]\n",
      "Output tensor shape:  (5, 4, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    h_conv_res = sess.run(h_conv)\n",
    "    print(h_conv_res)\n",
    "    print(\"Output tensor shape: \", h_conv_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'max_pooling2d/MaxPool:0' shape=(5, 2, 2, 2) dtype=float32>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Flatten last pooling 1-D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(5, 8) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_dim = h_pool.get_shape().as_list()[1] * h_pool.get_shape().as_list()[2] * h_pool.get_shape().as_list()[3]\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, flatten_dim])\n",
    "h_pool_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) FC layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([flatten_dim, 2], name = 'W')\n",
    "    b_fc1 = bias_variable([2], name = 'b')\n",
    "h_fc1 = tf.matmul(h_pool_flat, W_fc1) + b_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(5, 2) dtype=float32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_fc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Batch norm (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(h, is_training):\n",
    "    return tf.contrib.layers.batch_norm(h, \n",
    "                                        center=True, \n",
    "                                        scale=True, \n",
    "                                        is_training=is_training)\n",
    "h_fc1_batchnorm = batch_norm(h_fc1, is_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=tf.constant([[1,0], \n",
    "                                                                   [1,0], \n",
    "                                                                   [0,1], \n",
    "                                                                   [0,1], \n",
    "                                                                   [1,0]]),\n",
    "                                               logits=h_fc1)\n",
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Training Step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d/kernel:0' shape=(1, 1, 3, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d/bias:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'fc1/W:0' shape=(8, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'fc1/b:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'BatchNorm/beta:0' shape=(2,) dtype=float32_ref>,\n",
       " <tf.Variable 'BatchNorm/gamma:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Prediction and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = h_fc1\n",
    "y_target = np.array([[1,0], \n",
    "                     [1,0], \n",
    "                     [0,1], \n",
    "                     [0,1], \n",
    "                     [1,0]])\n",
    "probs = tf.nn.softmax(logits)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_target, 1))\n",
    "correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    accuracy = sess.run(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for a random prediction:  0.6\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for a random prediction: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The whole script for building graph - to understand ```train.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Build the CNN graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(config):\n",
    "    \"\"\"This function builds the graph for a deep net for classifying images.\n",
    "    Args:\n",
    "      config: Model configuration object\n",
    "    Returns:\n",
    "      A tuple (y, keep_prob). y is a tensor of shape (N_examples, 10), with values\n",
    "      equal to the logits of classifying the image into one of 10 classes (the\n",
    "      digits 0-9). keep_prob is a scalar placeholder for the probability of\n",
    "      dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    x_image = tf.placeholder(tf.float32, [None, config.image_height, config.image_width, config.image_channels])\n",
    "    y = tf.placeholder(tf.float32, [None, int(config.num_classes)])\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # First convolutional module [conv-conv-pool] -- maps one grayscale image to 32 feature maps.\n",
    "    with tf.name_scope('conv1_1'):\n",
    "        W_conv1_1 = weight_variable([config.filter_size, config.filter_size, config.image_channels, config.conv1_num_filters], name = 'W')\n",
    "        b_conv1_1 = bias_variable([config.conv1_num_filters], name = 'b')\n",
    "        c_conv1_1 = conv2d(x_image, W_conv1_1) + b_conv1_1\n",
    "        #c_conv1_1 = batch_norm1(c_conv1_1, is_training)\n",
    "        h_conv1_1 = tf.nn.relu(c_conv1_1)\n",
    "\n",
    "    with tf.name_scope('conv1_2'):\n",
    "        W_conv1_2 = weight_variable([config.filter_size, config.filter_size, config.conv1_num_filters, config.conv1_num_filters], name = 'W')\n",
    "        b_conv1_2 = bias_variable([config.conv1_num_filters], name = 'b')\n",
    "        c_conv1_2 = conv2d(h_conv1_1, W_conv1_2) + b_conv1_2\n",
    "        #c_conv1_2 = batch_norm1(c_conv1_2, is_training)\n",
    "        h_conv1_2 = tf.nn.relu(c_conv1_2)\n",
    "\n",
    "    # Pooling layer - downsamples by 2X.\n",
    "    with tf.name_scope('pool1'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1_2)\n",
    "\n",
    "    # Second convolutional module [conv-conv-pool] -- maps 32 feature maps to 64.\n",
    "    with tf.name_scope('conv2_1'):\n",
    "        W_conv2_1 = weight_variable([config.filter_size, config.filter_size, config.conv1_num_filters, config.conv2_num_filters], name = 'W')\n",
    "        b_conv2_1 = bias_variable([config.conv2_num_filters], name = 'b')\n",
    "        c_conv2_1 = conv2d(h_pool1, W_conv2_1) + b_conv2_1\n",
    "        #c_conv2_1 = batch_norm1(c_conv2_1, is_training)\n",
    "        h_conv2_1 = tf.nn.relu(c_conv2_1)\n",
    "\n",
    "    with tf.name_scope('conv2_2'):\n",
    "        W_conv2_2 = weight_variable([config.filter_size, config.filter_size, config.conv2_num_filters, config.conv2_num_filters], name = 'W')\n",
    "        b_conv2_2 = bias_variable([config.conv2_num_filters], name = 'b')\n",
    "        c_conv2_2 = conv2d(h_conv2_1, W_conv2_2) + b_conv2_2\n",
    "        #c_conv2_2 = batch_norm1(c_conv2_2, is_training)\n",
    "        h_conv2_2 = tf.nn.relu(c_conv2_2)\n",
    "        \n",
    "    # Second pooling layer.\n",
    "    with tf.name_scope('pool2'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2_2)\n",
    "\n",
    "    # Fully connected layer 1 -- after 2 round of downsampling, our 32x32 image\n",
    "    # is down to 4x4x64 feature maps -- maps this to 1024 features.\n",
    "    #feature_map_flattened_dim = int((config.image_height/(2**2)) * (config.image_width/(2**2)) * config.conv2_num_filters)\n",
    "    feature_map_flattened_dim = int(np.prod(h_pool2.get_shape()[1:]))\n",
    "\n",
    "    h_pool2_flat = tf.reshape(h_pool2, [-1, feature_map_flattened_dim])\n",
    "    with tf.name_scope('fc1'):\n",
    "        W_fc1 = weight_variable([feature_map_flattened_dim, config.fc1_num_features], name = 'W')\n",
    "        b_fc1 = bias_variable([config.fc1_num_features], name = 'b')\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "    with tf.name_scope('dropout1'):\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    \"\"\"with tf.name_scope('fc2'):\n",
    "        W_fc2 = weight_variable([config.fc1_num_features, config.fc2_num_features], name = 'W')\n",
    "        b_fc2 = bias_variable([config.fc2_num_features], name = 'b')\n",
    "    h_fc2 = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "    #h_fc2 = batch_norm1(h_fc2, is_training)\n",
    "    h_fc2 = tf.nn.relu(h_fc2)\n",
    "    with tf.name_scope('dropout2'):\n",
    "        h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\"\"\"\n",
    "\n",
    "    # Map the 1024 features to 10 classes, one for each digit\n",
    "    with tf.name_scope('fc3'):\n",
    "        W_fc3 = weight_variable([config.fc1_num_features, config.num_classes], name = 'W')\n",
    "        b_fc3 = bias_variable([config.num_classes], name = 'b')\n",
    "    \n",
    "    # Raw predictions - logits\n",
    "    with tf.name_scope('logits'):\n",
    "        logits = tf.matmul(h_fc1_drop, W_fc3) + b_fc3\n",
    "\n",
    "    with tf.name_scope('probabilities'):\n",
    "        probs = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                       logits=logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.name_scope('adam_optimizer'):\n",
    "            train_step = tf.train.AdamOptimizer(config.learning_rate).minimize(loss)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "    saver = tf.train.Saver\n",
    "\n",
    "    # Return the model in dict\n",
    "    return dict(\n",
    "        x_image = x_image, \n",
    "        y = y, \n",
    "        is_training = is_training, \n",
    "        keep_prob = keep_prob, \n",
    "        h_conv1_1 = h_conv1_1,\n",
    "        logits = logits, \n",
    "        probs = probs, \n",
    "        loss = loss, \n",
    "        train_step = train_step, \n",
    "        accuracy = accuracy, \n",
    "        saver = saver, \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_for_one_epoch(iterations, train_x, train_y, model, sess, config, record_train_loss = False):\n",
    "\n",
    "    num_images = len(train_x)\n",
    "    for i in range(iterations):\n",
    "        # Create a random index.\n",
    "        idx = np.random.choice(num_images,\n",
    "                               size=config.batch_size,\n",
    "                               replace=False)\n",
    "        batch_x = train_x[idx, :, :, :]\n",
    "        batch_y = train_y[idx, :]\n",
    "\n",
    "        \n",
    "        _, temp_loss = sess.run([model['train_step'], model['loss']], feed_dict={model['x_image']: batch_x, \\\n",
    "                                                                                 model['y']: batch_y, \\\n",
    "                                                                                 model['is_training']: True, \\\n",
    "                                                                                 model['keep_prob']:config.keep_rate})\n",
    "        if record_train_loss: \n",
    "            training_loss.append(temp_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_prediction(val_x, val_y, model, sess, list_to_pred):\n",
    "\n",
    "    predictions = sess.run([model[p] for p in list_to_pred], feed_dict={model['x_image']: val_x, \\\n",
    "                                                                        model['y']: val_y, \\\n",
    "                                                                        model['is_training']: False, \\\n",
    "                                                                        model['keep_prob']: 1.0})\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Visualization for loss accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & val losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/train_losses.png\" />\n",
    "\n",
    "<img src=\"images/val_losses_n_accuracy.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Visualize activations from images - cell below for explanation only, please don't run it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(train_data.shape[0],\n",
    "      size=1,\n",
    "      replace=False)\n",
    "img = train_data[idx[0], :, :, :]\n",
    "## visualize layer-1 kernel weights in grid \n",
    "img = np.expand_dims(img, 0)\n",
    "h_conv1_1 = sess.run(model['h_conv1_1'], feed_dict={model['x_image']:img})\n",
    "h_conv1_1 = h_conv1_1.transpose(3, 1, 2, 0)   # reshape to: (N, H, W, 1)\n",
    "vis_grid = visualize_grid(h_conv1_1, grey = True)\n",
    "plot_weights_in_grid(vis_grid, os.path.join(save_dir, 'vis_activations.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tr>\n",
    "<td><img src=\"images/vis_acti_dog.png\" /></td>\n",
    "<td><img src=\"images/vis_acti_frog.png\" /></td>\n",
    "</tr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the files: \n",
    "****`train.py`:****  To build & train the model, as well as to monitor the training progress and to visualize graphs like above. Upon completion, it will save the trained model into specified location. How to run:                        \n",
    "`python train.py --trainDir /home/ubuntu/workshop/ --savedSessionDir /home/ubuntu/workshop/savedSessions/`\n",
    "\n",
    "**** `build_graph.py`: ****  Build the model graph and return list of nodes to the graph\n",
    "\n",
    "**** `configuration.py`: **** Configuration file for model and training. \n",
    "\n",
    "**** `vis_utils.py`: **** Visualization utility functions\n",
    "\n",
    "**** `data_utils.py`: **** Data utility functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
